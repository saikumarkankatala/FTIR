# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iXnfUT9NsTjcSqzgJGxbv5U_rs36YDfo
"""
import pandas as pd
from keras.src.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam



import numpy as np

# Load the dataset
data_path = 'Full_Data_Raw.csv'
data = pd.read_csv(data_path)

# Assuming the last column is the target variable and others are features
X = data.iloc[:, :-1].values  # Features
y = data.iloc[:, -1].values  # Labels

# Reshape X to fit the model input requirements: [samples, time steps, features]
X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))

# Convert the labels to one-hot encoding
y_one_hot = to_categorical(y)

X_train, X_test, y_train, y_test = train_test_split(
    X_reshaped,
    y_one_hot,
    test_size=0.2,
    random_state=45,
    shuffle=True
)

# Define the 1D CNN model
model = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),
    MaxPooling1D(pool_size=2),
    Dropout(0.5),
    Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),
    Flatten(),
    Dense(100, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(y_one_hot.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

plot_data = model.fit(X_train, y_train, epochs=30, batch_size=22, validation_split=0.2)

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc*100:.2f}%")

# Predict and generate the classification report
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)
cm = confusion_matrix(true_classes, predicted_classes)

report = classification_report(true_classes, predicted_classes, target_names=['PS', 'PET', 'LDPE', 'PVC'])
print(report)

per_class_accuracy = cm.diagonal() / cm.sum(axis=1)
for idx, cls in enumerate(['PS', 'PET', 'LDPE', 'PVC']):
    print(f"Accuracy for class {cls}: {per_class_accuracy[idx]*100:.2f}%")

# Plotting training and validation loss
plt.plot(plot_data.history['loss'], label='Training loss')
plt.plot(plot_data.history['val_loss'], label='Validation loss')
plt.title('Training and Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend()
plt.show()

plt.plot(plot_data.history['accuracy'], label='Training accuracy')
plt.plot(plot_data.history['val_accuracy'], label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend()
plt.show()

